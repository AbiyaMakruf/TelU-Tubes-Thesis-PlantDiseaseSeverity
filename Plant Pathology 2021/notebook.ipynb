{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42aadcc9",
   "metadata": {},
   "source": [
    "# Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b155fc2",
   "metadata": {},
   "source": [
    "## Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685e7c1",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.data.annotator import auto_annotate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf4078",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Object Detection\n",
    "rf = Roboflow(api_key=\"MQdx0fMQ8FiQPaS1VHRH\")\n",
    "project = rf.workspace(\"abiya-thesis\").project(\"plant-pathology-2021-object-detection-j1jvh\")\n",
    "version = project.version(9)\n",
    "dataset = version.download(\"yolov11\", location=\"dataset/object_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Mask Lesi\n",
    "rf = Roboflow(api_key=\"MQdx0fMQ8FiQPaS1VHRH\")\n",
    "project = rf.workspace(\"abiya-thesis\").project(\"plant-pathology-2021-instance-segmentation-h5pim\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"yolov11\", location=\"dataset/mask_lesi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3821de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Mask Lesi and Leaf\n",
    "rf = Roboflow(api_key=\"MQdx0fMQ8FiQPaS1VHRH\")\n",
    "project = rf.workspace(\"abiya-thesis\").project(\"plant-pathology-2021-instance-segmentation-h5pim\")\n",
    "version = project.version(5)\n",
    "dataset = version.download(\"yolov11\", location=\"dataset/mask_lesi_and_leaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acae910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Leaf Class Only\n",
    "rf = Roboflow(api_key=\"MQdx0fMQ8FiQPaS1VHRH\")\n",
    "project = rf.workspace(\"abiya-thesis\").project(\"plant-pathology-2021-object-detection-j1jvh\")\n",
    "version = project.version(11)\n",
    "dataset = version.download(\"yolov11\", location=\"dataset/object_detection_leaf_class_only\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903a1d6",
   "metadata": {},
   "source": [
    "## Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dba498",
   "metadata": {},
   "outputs": [],
   "source": [
    "GoogleDrive = True\n",
    "if GoogleDrive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    def copy_to_google_drive(source_folder, destination_folder):\n",
    "        !cp -r /content/{source_folder} /content/gdrive/MyDrive/{destination_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8457ccb",
   "metadata": {},
   "source": [
    "## Global Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_and_save_time(start_time, output_path):\n",
    "    elapsed = time.time() - start_time\n",
    "    h, rem = divmod(elapsed, 3600)\n",
    "    m, s = divmod(rem, 60)\n",
    "    formatted = f\"{int(h)}h {int(m)}m {int(s)}s\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed257141",
   "metadata": {},
   "source": [
    "## Training Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = ['yolo11n.pt', 'yolo11s.pt', 'yolo11m.pt']\n",
    "sizes = ['nano', 'small', 'medium']\n",
    "epochs = 100\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_objectDetection = \"dataset/object_detection\"\n",
    "project_base_objectDetection = \"results/object_detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, size in zip(models, sizes):\n",
    "    print(f\"Training {size} model...\")\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    start_time = time.time()\n",
    "    training = model.train(\n",
    "        data=f\"{data_path_objectDetection}/data.yaml\", \n",
    "        epochs=epochs, \n",
    "        imgsz=640, \n",
    "        batch=batch, \n",
    "        project=f\"{project_base_objectDetection}/training\", \n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True\n",
    "    )\n",
    "\n",
    "    measure_and_save_time(start_time, f\"{project_base_objectDetection}/training/summary/time/{size}-train-time.txt\")\n",
    "    csv_filename = f\"{project_base_objectDetection}/training/summary/{size}-training-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(training.to_csv())\n",
    "\n",
    "    validation = model.val(\n",
    "        data=f\"{data_path_objectDetection}/data.yaml\", \n",
    "        imgsz=640,\n",
    "        project=f\"{project_base_objectDetection}/validation\",\n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True,\n",
    "        split=\"test\"\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_objectDetection}/validation/summary/{size}-validation-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(validation.to_csv())\n",
    "\n",
    "    print(f\"Finished training and validating {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    print(f\"Running prediction for {size} model...\")\n",
    "\n",
    "    model = YOLO(f\"results/object_detection/training/{size}/weights/best.pt\")\n",
    "\n",
    "    # Define the output directory for the current model size\n",
    "    output_dir = f\"results/object_detection/predict/{size}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = model(\n",
    "        source=f\"{data_path_objectDetection}/test/images\", \n",
    "        exist_ok=True,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        original_filename = os.path.basename(result.path)\n",
    "        save_path = os.path.join(output_dir, original_filename)\n",
    "        result.save(filename=save_path)\n",
    "\n",
    "    print(f\"Finished predicting for {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930929bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for size in sizes:\n",
    "    df = pd.read_csv(f\"{project_base_objectDetection}/training/{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - Model {size}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_objectDetection}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/object_detection\", \"dataset/object_detection\")\n",
    "    copy_to_google_drive(\"results/object_detection\", \"results/object_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12cf94",
   "metadata": {},
   "source": [
    "## Semi Auto Annotate Mask Daun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e26998",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"train\",\"valid\",'test']\n",
    "sam2_model = \"sam2.1_b.pt\"\n",
    "data_path_maskDaun = \"dataset/mask_daun\"\n",
    "project_base_maskDaun = \"results/mask_daun\"\n",
    "best_objectDetection_model_size = \"medium\"\n",
    "best_objectDetection_model_path = f\"{project_base_objectDetection}/training/{best_objectDetection_model_size}/weights/best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ea408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pindahkan original images ke mask daun\n",
    "for split in splits:\n",
    "    shutil.copytree(src=f\"{data_path_objectDetection}/{split}/images\", dst=f\"{data_path_maskDaun}/{split}/images\", dirs_exist_ok=True)\n",
    "\n",
    "shutil.copy(src=f\"{data_path_objectDetection}/data.yaml\", dst=f\"{data_path_maskDaun}/data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba7b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    auto_annotate(data=f\"{data_path_maskDaun}/{split}/images/\", det_model=best_objectDetection_model_path, sam_model=sam2_model, output_dir=f\"{data_path_maskDaun}/{split}/labels/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5a7fa",
   "metadata": {},
   "source": [
    "## Training Mask Daun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f541595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = ['yolo11n-seg.pt', 'yolo11s-seg.pt', 'yolo11m-seg.pt']\n",
    "sizes = ['nano', 'small', 'medium']\n",
    "epochs = 100\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_maskDaun = \"dataset/mask_daun\"\n",
    "project_base_maskDaun = \"results/mask_daun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, size in zip(models, sizes):\n",
    "    print(f\"Training {size} model...\")\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    start_time = time.time()\n",
    "    training = model.train(\n",
    "        data=f\"{data_path_maskDaun}/data.yaml\", \n",
    "        epochs=epochs, \n",
    "        imgsz=640, \n",
    "        batch=batch, \n",
    "        project=f\"{project_base_maskDaun}/training\", \n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True\n",
    "    )\n",
    "\n",
    "    measure_and_save_time(start_time, f\"{project_base_maskDaun}/training/summary/time/{size}-train-time.txt\")\n",
    "    csv_filename = f\"{project_base_maskDaun}/training/summary/{size}-training-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(training.to_csv())\n",
    "\n",
    "    validation = model.val(\n",
    "        data=f\"{data_path_maskDaun}/data.yaml\", \n",
    "        imgsz=640,\n",
    "        project=f\"{project_base_maskDaun}/validation\",\n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True,\n",
    "        split=\"test\"\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_maskDaun}/validation/summary/{size}-validation-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(validation.to_csv())\n",
    "\n",
    "    print(f\"Finished training and validating {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b01225",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    print(f\"Running prediction for {size} model...\")\n",
    "\n",
    "    model = YOLO(f\"results/mask_daun/training/{size}/weights/best.pt\")\n",
    "\n",
    "    # Define the output directory for the current model size\n",
    "    output_dir = f\"results/mask_daun/predict/{size}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = model(\n",
    "        source=f\"{data_path_maskDaun}/test/images\", \n",
    "        exist_ok=True,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        original_filename = os.path.basename(result.path)\n",
    "        save_path = os.path.join(output_dir, original_filename)\n",
    "        result.save(filename=save_path)\n",
    "\n",
    "    print(f\"Finished predicting for {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99138daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for size in sizes:\n",
    "    df = pd.read_csv(f\"{project_base_maskDaun}/training/{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - Model {size}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_maskDaun}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005aad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/mask_daun\", \"dataset/mask_daun\")\n",
    "    copy_to_google_drive(\"results/mask_daun\", \"results/mask_daun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d607f9",
   "metadata": {},
   "source": [
    "## Training Mask Lesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = ['yolo11n-seg.pt', 'yolo11s-seg.pt', 'yolo11m-seg.pt']\n",
    "sizes = ['nano', 'small', 'medium']\n",
    "epochs = 100\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_maskLesi = \"dataset/mask_lesi\"\n",
    "project_base_maskLesi = \"results/mask_lesi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, size in zip(models, sizes):\n",
    "    print(f\"Training {size} model...\")\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    start_time = time.time()\n",
    "    training = model.train(\n",
    "        data=f\"{data_path_maskLesi}/data.yaml\", \n",
    "        epochs=epochs, \n",
    "        imgsz=640, \n",
    "        batch=batch, \n",
    "        project=f\"{project_base_maskLesi}/training\", \n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True\n",
    "    )\n",
    "\n",
    "    measure_and_save_time(start_time, f\"{project_base_maskLesi}/training/summary/time/{size}-train-time.txt\")\n",
    "    csv_filename = f\"{project_base_maskLesi}/training/summary/{size}-training-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(training.to_csv())\n",
    "\n",
    "    validation = model.val(\n",
    "        data=f\"{data_path_maskLesi}/data.yaml\", \n",
    "        imgsz=640,\n",
    "        project=f\"{project_base_maskLesi}/validation\",\n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True,\n",
    "        split=\"test\"\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_maskLesi}/validation/summary/{size}-validation-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(validation.to_csv())\n",
    "\n",
    "    print(f\"Finished training and validating {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de773ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    print(f\"Running prediction for {size} model...\")\n",
    "\n",
    "    model = YOLO(f\"results/mask_lesi/training/{size}/weights/best.pt\")\n",
    "\n",
    "    # Define the output directory for the current model size\n",
    "    output_dir = f\"results/mask_lesi/predict/{size}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = model(\n",
    "        source=f\"{data_path_maskLesi}/test/images\", # Kedepannya ubah menjadi /test jika sudah ada test yang di anotasi\n",
    "        exist_ok=True,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        original_filename = os.path.basename(result.path)\n",
    "        save_path = os.path.join(output_dir, original_filename)\n",
    "        result.save(filename=save_path)\n",
    "\n",
    "    print(f\"Finished predicting for {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for size in sizes:\n",
    "    df = pd.read_csv(f\"{project_base_maskLesi}/training/{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - Model {size}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_maskLesi}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02fb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/mask_lesi\", \"dataset/mask_lesi\")\n",
    "    copy_to_google_drive(\"results/mask_lesi\", \"results/mask_lesi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b33735b",
   "metadata": {},
   "source": [
    "## Training Mask Lesi dan Daun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = ['yolo11n-seg.pt', 'yolo11s-seg.pt', 'yolo11m-seg.pt']\n",
    "sizes = ['nano', 'small', 'medium']\n",
    "epochs = 100\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_maskLesi_and_maskLeaf = \"dataset/mask_lesi_and_leaf\"\n",
    "project_base_maskLesi_and_maskLeaf = \"results/mask_lesi_and_leaf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, size in zip(models, sizes):\n",
    "    print(f\"Training {size} model...\")\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    start_time = time.time()\n",
    "    training = model.train(\n",
    "        data=f\"{data_path_maskLesi_and_maskLeaf}/data.yaml\", \n",
    "        epochs=epochs, \n",
    "        imgsz=640, \n",
    "        batch=batch, \n",
    "        project=f\"{project_base_maskLesi_and_maskLeaf}/training\", \n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True\n",
    "    )\n",
    "\n",
    "    measure_and_save_time(start_time, f\"{project_base_maskLesi_and_maskLeaf}/training/summary/time/{size}-train-time.txt\")\n",
    "    csv_filename = f\"{project_base_maskLesi_and_maskLeaf}/training/summary/{size}-training-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(training.to_csv())\n",
    "\n",
    "    validation = model.val(\n",
    "        data=f\"{data_path_maskLesi_and_maskLeaf}/data.yaml\", \n",
    "        imgsz=640,\n",
    "        project=f\"{project_base_maskLesi}/validation\",\n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True,\n",
    "        split=\"test\"\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_maskLesi}/validation/summary/{size}-validation-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(validation.to_csv())\n",
    "\n",
    "    print(f\"Finished training and validating {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    print(f\"Running prediction for {size} model...\")\n",
    "\n",
    "    model = YOLO(f\"results/mask_lesi/training/{size}/weights/best.pt\")\n",
    "\n",
    "    # Define the output directory for the current model size\n",
    "    output_dir = f\"results/mask_lesi/predict/{size}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = model(\n",
    "        source=f\"{data_path_maskLesi_and_maskLeaf}/test/images\",\n",
    "        exist_ok=True,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        original_filename = os.path.basename(result.path)\n",
    "        save_path = os.path.join(output_dir, original_filename)\n",
    "        result.save(filename=save_path)\n",
    "\n",
    "    print(f\"Finished predicting for {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f47165",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for size in sizes:\n",
    "    df = pd.read_csv(f\"{project_base_maskLesi_and_maskLeaf}/training/{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - Model {size}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_maskLesi_and_maskLeaf}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/mask_lesi_and_leaf\", \"dataset/mask_lesi_and_leaf\")\n",
    "    copy_to_google_drive(\"results/mask_lesi_and_leaf\", \"results/mask_lesi_and_leaf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876eb91e",
   "metadata": {},
   "source": [
    "## Object Detection Class Leaf Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = ['yolo11n.pt', 'yolo11s.pt', 'yolo11m.pt']\n",
    "sizes = ['nano', 'small', 'medium']\n",
    "epochs = 100\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_objectDetection_leaf_class_only = \"dataset/object_detection_leaf_class_only\"\n",
    "project_base_objectDetection_leaf_class_only = \"results/object_detection_leaf_class_only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, size in zip(models, sizes):\n",
    "    print(f\"Training {size} model...\")\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    training = model.train(\n",
    "        data=f\"{data_path_objectDetection_leaf_class_only}/data.yaml\", \n",
    "        epochs=epochs, \n",
    "        imgsz=640, \n",
    "        batch=batch, \n",
    "        project=f\"{project_base_objectDetection_leaf_class_only}/training\", \n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_objectDetection_leaf_class_only}/training/summary/{size}-training-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(training.to_csv())\n",
    "\n",
    "    validation = model.val(\n",
    "        data=f\"{data_path_objectDetection_leaf_class_only}/data.yaml\", \n",
    "        imgsz=640,\n",
    "        project=f\"{project_base_objectDetection_leaf_class_only}/validation\",\n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True,\n",
    "        split=\"test\"\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_objectDetection_leaf_class_only}/validation/summary/{size}-validation-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(validation.to_csv())\n",
    "\n",
    "    print(f\"Finished training and validating {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    print(f\"Running prediction for {size} model...\")\n",
    "\n",
    "    model = YOLO(f\"results/object_detection_leaf_class_only/training/{size}/weights/best.pt\")\n",
    "\n",
    "    # Define the output directory for the current model size\n",
    "    output_dir = f\"results/object_detection_leaf_class_only/predict/{size}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = model(\n",
    "        source=f\"{data_path_objectDetection_leaf_class_only}/test/images\", \n",
    "        exist_ok=True,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        original_filename = os.path.basename(result.path)\n",
    "        save_path = os.path.join(output_dir, original_filename)\n",
    "        result.save(filename=save_path)\n",
    "\n",
    "    print(f\"Finished predicting for {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f605f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for size in sizes:\n",
    "    df = pd.read_csv(f\"{project_base_objectDetection_leaf_class_only}/training/{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - Model {size}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_objectDetection_leaf_class_only}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4235f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/object_detection_leaf_class_only\", \"dataset/object_detection_leaf_class_only\")\n",
    "    copy_to_google_drive(\"results/object_detection_leaf_class_only\", \"results/object_detection_leaf_class_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b50cb",
   "metadata": {},
   "source": [
    "## Fine Tune For Detect Healthy, Rust, and Scab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb41dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path sumber dataset YOLO\n",
    "image_dir = Path(\"dataset/object_detection/train/images\")\n",
    "label_dir = Path(\"dataset/object_detection/train/labels\")\n",
    "\n",
    "# Path target few-shot\n",
    "few_shot_base = Path(\"dataset/few_shot\")\n",
    "\n",
    "# Mapping class ID ke nama\n",
    "class_map = {0: \"frog-eye-leaf-spot\", 1: \"healthy\", 2: \"rust\"}\n",
    "\n",
    "# Buat index gambar per class berdasarkan nama file\n",
    "class_to_files = defaultdict(list)\n",
    "for img_file in sorted(image_dir.glob(\"*.jpg\")):\n",
    "    filename_lower = img_file.name.lower()\n",
    "    if \"frog-eye-leaf-spot\" in filename_lower:\n",
    "        cls = 0\n",
    "    elif \"healthy\" in filename_lower:\n",
    "        cls = 1\n",
    "    elif \"rust\" in filename_lower:\n",
    "        cls = 2\n",
    "    else:\n",
    "        continue  # Skip jika tidak dikenali\n",
    "\n",
    "    label_file = label_dir / (img_file.stem + \".txt\")\n",
    "    if label_file.exists():\n",
    "        class_to_files[cls].append((img_file, label_file))\n",
    "\n",
    "# Generate few-shot data\n",
    "for n in [5] + list(range(10, 51, 5)):\n",
    "    target_image_dir = few_shot_base / f\"{n}-shot\" / \"images\"\n",
    "    target_label_dir = few_shot_base / f\"{n}-shot\" / \"labels\"\n",
    "    target_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "    target_label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for cls in class_map:\n",
    "        files = class_to_files[cls][:n]  # ambil berurutan\n",
    "        for img_path, label_path in files:\n",
    "            shutil.copy(img_path, target_image_dir / img_path.name)\n",
    "            shutil.copy(label_path, target_label_dir / label_path.name)\n",
    "\n",
    "# Copy data.yaml\n",
    "shutil.copy(\"dataset/object_detection/data.yaml\", \"dataset/few_shot/data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = {\"nano\": \"results/object_detection_leaf_class_only/training/nano/weights/best.pt\",\n",
    "          \"small\": \"results/object_detection_leaf_class_only/training/small/weights/best.pt\",\n",
    "          \"medium\": \"results/object_detection_leaf_class_only/training/medium/weights/best.pt\"}\n",
    "sizes = [\"nano\", \"small\", \"medium\"]\n",
    "shots = [5] + list(range(10, 51, 5))\n",
    "size_to_shots = {\n",
    "    \"nano\":   shots,\n",
    "    \"small\":  shots,\n",
    "    \"medium\": shots\n",
    "}\n",
    "epochs = 50\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_few_shot = \"dataset/few_shot\"\n",
    "project_base_few_shot = \"results/few_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    for shot in size_to_shots[size]:\n",
    "        # 1. Load original YAML\n",
    "        data_yaml_path = f\"{data_path_few_shot}/data.yaml\"\n",
    "        with open(data_yaml_path, 'r') as f:\n",
    "            data_yaml = yaml.safe_load(f)\n",
    "\n",
    "        # 2. Ubah path train sesuai shot\n",
    "        data_yaml['train'] = f\"../{shot}-shot/images\"\n",
    "\n",
    "        # 3. Simpan YAML baru ke file sementara\n",
    "        temp_yaml_path = f\"{data_path_few_shot}/data-shot.yaml\"\n",
    "        with open(temp_yaml_path, 'w') as f:\n",
    "            yaml.dump(data_yaml, f, sort_keys=False, default_flow_style=True)\n",
    "\n",
    "        model = YOLO(models[size])\n",
    "        start_time = time.time()\n",
    "        training = model.train(\n",
    "            data = temp_yaml_path,\n",
    "            epochs = epochs,\n",
    "            imgsz=640,\n",
    "            batch=batch,\n",
    "            project=f\"{project_base_few_shot}/training\",\n",
    "            name=f\"{shot}-shot-{size}\",\n",
    "            exist_ok=True,\n",
    "            # patience=25,\n",
    "            # freeze=8\n",
    "        )\n",
    "\n",
    "        csv_filename = f\"{project_base_few_shot}/training/summary/{shot}-shot-{size}-training-metrics.csv\"\n",
    "        os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "        with open(csv_filename, \"w\") as f:\n",
    "            f.write(training.to_csv())\n",
    "\n",
    "        validation = model.val(\n",
    "            data = temp_yaml_path,\n",
    "            imgsz=640,\n",
    "            project=f\"{project_base_few_shot}/validation\",\n",
    "            name=f\"{shot}-shot-{size}\",\n",
    "            exist_ok=True,\n",
    "            split=\"test\"\n",
    "        )\n",
    "\n",
    "        csv_filename = f\"{project_base_few_shot}/validation/summary/{shot}-shot-{size}-validation-metrics.csv\"\n",
    "        os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "        with open(csv_filename, \"w\") as f:\n",
    "            f.write(validation.to_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for shot in shots:\n",
    "    df = pd.read_csv(f\"{project_base_few_shot}/training/{shot}-shot-{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - {size} | {shot}-shot\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_few_shot}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{shot}-shot-{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33725b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/few_shot\", \"dataset/few_shot\")\n",
    "    copy_to_google_drive(\"results/few_shot\", \"results/few_shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8bc06",
   "metadata": {},
   "source": [
    "## Perbandingan Beberapa k-query Set vs n-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeadb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path sumber dataset YOLO\n",
    "image_dir = Path(\"dataset/object_detection/train/images\")\n",
    "label_dir = Path(\"dataset/object_detection/train/labels\")\n",
    "\n",
    "# Path target support-set\n",
    "support_set_base = Path(\"dataset/support_set\")\n",
    "\n",
    "# Mapping class ID ke nama\n",
    "class_map = {0: \"frog-eye-leaf-spot\", 1: \"healthy\", 2: \"rust\"}\n",
    "\n",
    "# Buat index gambar per class berdasarkan nama file\n",
    "class_to_files = defaultdict(list)\n",
    "for img_file in sorted(image_dir.glob(\"*.jpg\")):\n",
    "    filename_lower = img_file.name.lower()\n",
    "    if \"frog-eye-leaf-spot\" in filename_lower:\n",
    "        cls = 0\n",
    "    elif \"healthy\" in filename_lower:\n",
    "        cls = 1\n",
    "    elif \"rust\" in filename_lower:\n",
    "        cls = 2\n",
    "    else:\n",
    "        continue  # Skip jika tidak dikenali\n",
    "\n",
    "    label_file = label_dir / (img_file.stem + \".txt\")\n",
    "    if label_file.exists():\n",
    "        class_to_files[cls].append((img_file, label_file))\n",
    "\n",
    "# Generate support-set data\n",
    "for n in [5] + list(range(10, 51, 5)):\n",
    "    target_image_dir = support_set_base / f\"{n}-images\" / \"images\"\n",
    "    target_label_dir = support_set_base / f\"{n}-images\" / \"labels\"\n",
    "    target_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "    target_label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for cls in class_map:\n",
    "        files = class_to_files[cls][-n:]  # ambil n gambar terakhir\n",
    "        for img_path, label_path in files:\n",
    "            shutil.copy(img_path, target_image_dir / img_path.name)\n",
    "            shutil.copy(label_path, target_label_dir / label_path.name)\n",
    "\n",
    "# Copy data.yaml\n",
    "shutil.copy(\"dataset/object_detection/data.yaml\", \"dataset/support_set/data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Variabel Global (disesuaikan dengan training sebelumnya)\n",
    "# ======================================================\n",
    "sizes = [\"nano\", \"small\", \"medium\"]\n",
    "shots = [5] + list(range(10, 51, 5))\n",
    "project_base_few_shot = \"results/few_shot\"\n",
    "support_set_base = \"dataset/support_set\"\n",
    "\n",
    "# ======================================================\n",
    "# Setup data.yaml original (support_set)\n",
    "# ======================================================\n",
    "data_yaml_original_path = f\"{support_set_base}/data.yaml\"\n",
    "temp_data_yaml_dir = os.path.dirname(data_yaml_original_path)\n",
    "os.makedirs(temp_data_yaml_dir, exist_ok=True)\n",
    "\n",
    "# ======================================================\n",
    "# Dictionary hasil evaluasi\n",
    "# ======================================================\n",
    "all_map50_scores = {size: {} for size in sizes}\n",
    "\n",
    "# ======================================================\n",
    "# Loop evaluasi setiap model few-shot\n",
    "# ======================================================\n",
    "for size in sizes:\n",
    "    for shot in shots:\n",
    "        print(f\"\\nValidasi {size} model ({shot}-shot) ...\")\n",
    "\n",
    "        # Path model hasil training few-shot\n",
    "        model_path = f\"{project_base_few_shot}/training/{shot}-shot-{size}/weights/best.pt\"\n",
    "        model = YOLO(model_path)\n",
    "\n",
    "        # Load YAML original\n",
    "        with open(data_yaml_original_path, 'r') as f:\n",
    "            data_yaml = yaml.safe_load(f)\n",
    "\n",
    "        # Set path test â†’ support-set tertentu\n",
    "        data_yaml['test'] = f\"../{shot}-images/images\"\n",
    "        data_yaml['val']  = f\"../{shot}-images/images\"\n",
    "        data_yaml['train'] = f\"../{shot}-images/images\"\n",
    "\n",
    "        # Simpan yaml sementara\n",
    "        temp_yaml_path = os.path.join(temp_data_yaml_dir, f\"data_eval_{shot}-shot-{size}.yaml\")\n",
    "        with open(temp_yaml_path, 'w') as f:\n",
    "            yaml.dump(data_yaml, f, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "        # Validasi model\n",
    "        results = model.val(data=temp_yaml_path, \n",
    "                split=\"test\",\n",
    "                imgsz=640,\n",
    "                project=f\"{project_base_few_shot}/validation\",\n",
    "                name=f\"{shot}-shot-{size}\",\n",
    "                exist_ok=True)\n",
    "        map50 = results.results_dict.get(\"metrics/mAP50(B)\", 0.0)\n",
    "        all_map50_scores[size][shot] = map50\n",
    "\n",
    "        # Hapus folder runs agar bersih untuk next loop\n",
    "        if os.path.exists(\"runs\"):\n",
    "            shutil.rmtree(\"runs\")\n",
    "\n",
    "# ======================================================\n",
    "# Visualisasi Hasil\n",
    "# ======================================================\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title(\"Perbandingan mAP@0.5 Few-Shot per Model dan Ukuran Dataset Uji\")\n",
    "plt.xlabel(\"Jumlah Gambar per Kelas (Support-Set)\")\n",
    "plt.ylabel(\"mAP@0.5\")\n",
    "plt.xticks(shots)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot setiap model (nano, small, medium)\n",
    "for size in sizes:\n",
    "    sorted_scores = sorted(all_map50_scores[size].items())\n",
    "    x = [item[0] for item in sorted_scores]\n",
    "    y = [item[1] for item in sorted_scores]\n",
    "    plt.plot(x, y, marker='o', linestyle='-', label=f\"{size} model\")\n",
    "\n",
    "plt.legend(title=\"Model\", loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ======================================================\n",
    "# Cetak hasil lengkap\n",
    "# ======================================================\n",
    "print(\"\\n--- Hasil Lengkap mAP@0.5 ---\")\n",
    "for size in sizes:\n",
    "    print(f\"Model {size}:\")\n",
    "    for shot, score in all_map50_scores[size].items():\n",
    "        print(f\"  - {shot}-shot diuji di {shot}-images: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "# import matplotlib.pyplot as plt\n",
    "# from ultralytics import YOLO\n",
    "# import os\n",
    "# import shutil # Import shutil untuk operasi penghapusan folder\n",
    "\n",
    "# # Define the models to be evaluated and the test datasets\n",
    "# # Models trained with these 'shot' counts\n",
    "# lists_model_shots = [5] + list(range(10, 51, 5))\n",
    "# # Test datasets with these 'image' counts per class\n",
    "# lists_test_dataset_images = [5] + list(range(10, 51, 5))\n",
    "\n",
    "# # Dictionary to store mAP@0.5 scores for each model-test_dataset combination\n",
    "# # Format: {model_shot_count: {test_dataset_image_count: mAP50_score}}\n",
    "# all_map50_scores = {}\n",
    "\n",
    "# # Path to your original data.yaml file\n",
    "# data_yaml_original_path = \"dataset/support_set/data.yaml\"\n",
    "# # **MODIFIKASI DI SINI:** Temporary directory to save modified data.yaml files\n",
    "# # Sekarang akan disimpan di direktori yang sama dengan data_yaml_original_path\n",
    "# temp_data_yaml_dir = os.path.dirname(data_yaml_original_path)\n",
    "# # Default base directory for Ultralytics YOLO runs\n",
    "# yolo_runs_dir = \"runs\" # This is the default folder where YOLO saves validation results\n",
    "\n",
    "# # Ensure the temporary directory for YAMLs exists (though it should for support_set)\n",
    "# os.makedirs(temp_data_yaml_dir, exist_ok=True)\n",
    "\n",
    "# # Function to clear the runs/detect folder\n",
    "# def clear_yolo_runs_directory(base_dir=\".\"):\n",
    "#     \"\"\"\n",
    "#     Menghapus folder 'runs/detect' dari direktori dasar yang diberikan.\n",
    "#     \"\"\"\n",
    "#     detect_run_path = os.path.join(base_dir, yolo_runs_dir, 'detect')\n",
    "#     if os.path.exists(detect_run_path):\n",
    "#         print(f\"Menghapus direktori: {detect_run_path}\")\n",
    "#         try:\n",
    "#             shutil.rmtree(detect_run_path)\n",
    "#             print(f\"Direktori {detect_run_path} berhasil dihapus.\")\n",
    "#         except OSError as e:\n",
    "#             print(f\"Error: Gagal menghapus direktori {detect_run_path}. {e}\")\n",
    "#     else:\n",
    "#         print(f\"Direktori {detect_run_path} tidak ditemukan, tidak perlu dihapus.\")\n",
    "\n",
    "# # --- Start of Evaluation Loop ---\n",
    "# # Loop through each trained model\n",
    "# for model_shot_count in lists_model_shots:\n",
    "#     print(f\"\\n--- Testing Model: {model_shot_count}-shot ---\")\n",
    "\n",
    "#     # Clear YOLO runs directory before validating each model\n",
    "#     # This ensures a clean slate for each model's validation across test sets\n",
    "#     clear_yolo_runs_directory()\n",
    "\n",
    "#     # Initialize a dictionary to store scores for the current model\n",
    "#     all_map50_scores[model_shot_count] = {}\n",
    "\n",
    "#     # # Load the trained model\n",
    "#     model_path = f\"results/few_shot/training/{model_shot_count}-shot/weights/best.pt\"\n",
    "#     # try:\n",
    "#     #     model = YOLO(model_path)\n",
    "#     # except Exception as e:\n",
    "#     #     print(f\"Error loading model {model_path}: {e}. Skipping this model.\")\n",
    "#     #     continue # Skip to the next model if loading fails\n",
    "\n",
    "#     # Loop through each test dataset\n",
    "#     for test_image_count in lists_test_dataset_images:\n",
    "#         model = YOLO(model_path)\n",
    "#         # 1. Load the original data.yaml\n",
    "#         with open(data_yaml_original_path, 'r') as f:\n",
    "#             data_yaml = yaml.safe_load(f)\n",
    "\n",
    "#         # 2. Update the 'test' path to the current test dataset\n",
    "#         # This path is relative to the data.yaml itself.\n",
    "#         # Ensure that 'dataset/support_set' is the base for 'X-images'\n",
    "#         # For example, if data.yaml is in 'dataset/support_set/',\n",
    "#         # and images are in 'dataset/support_set/5-images/',\n",
    "#         # then the relative path from data.yaml to 5-images is '../5-images/images'\n",
    "#         # This relative path needs to be correct based on your actual file structure.\n",
    "#         data_yaml['test'] = f\"../{test_image_count}-images/images\"\n",
    "#         data_yaml['train'] = f\"../{test_image_count}-images/images\"\n",
    "#         data_yaml['val'] = f\"../{test_image_count}-images/images\"\n",
    "\n",
    "\n",
    "#         # 3. Save the modified YAML to a temporary file in the specified directory\n",
    "#         # **MODIFIKASI DI SINI:** temp_yaml_path sekarang menunjuk ke data_yaml_original_path's directory\n",
    "#         temp_yaml_filename = f\"data_support_test_{model_shot_count}model_{test_image_count}.yaml\"\n",
    "#         temp_yaml_path = os.path.join(temp_data_yaml_dir, temp_yaml_filename)\n",
    "#         with open(temp_yaml_path, 'w') as f:\n",
    "#             yaml.dump(data_yaml, f, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "#         # 4. Perform model validation\n",
    "#         print(f\"  Validating {model_shot_count}-shot model on {test_image_count}-images dataset...\")\n",
    "#         try:\n",
    "#             # Pass the path to the newly created temporary YAML file\n",
    "#             print(\"temp_yaml_path\",temp_yaml_path)\n",
    "#             results = model.val(data=temp_yaml_path)\n",
    "#             # Get mAP@0.5 score\n",
    "#             map50 = results.results_dict.get(\"metrics/mAP50(B)\", 0.0)\n",
    "#             all_map50_scores[model_shot_count][test_image_count] = map50\n",
    "#             print(f\"    mAP@0.5: {map50:.4f}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error validating {model_shot_count}-shot model on {test_image_count}-images: {e}\")\n",
    "#             all_map50_scores[model_shot_count][test_image_count] = 0.0 # Set to 0 if an error occurs\n",
    "\n",
    "#         # Optional: Remove the temporary YAML file after use\n",
    "#         # os.rmdir(\"runs/\")\n",
    "#         shutil.rmtree(\"runs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# # --- Visualization of Results ---\n",
    "# plt.figure(figsize=(15, 8))\n",
    "# plt.title(\"Perbandingan mAP@0.5 Model Few-Shot pada Berbagai Ukuran Dataset Pengujian\")\n",
    "# plt.xlabel(\"Ukuran Dataset Pengujian (Jumlah Gambar per Kelas)\")\n",
    "# plt.ylabel(\"mAP@0.5\")\n",
    "# plt.xticks(lists_test_dataset_images)\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# # Plot a line for each trained model\n",
    "# for model_shot_count, test_scores_dict in all_map50_scores.items():\n",
    "#     # Sort the test scores by dataset size for proper plotting\n",
    "#     sorted_test_scores = sorted(test_scores_dict.items())\n",
    "\n",
    "#     # Extract test dataset image counts and corresponding mAP50 scores\n",
    "#     test_dataset_sizes = [item[0] for item in sorted_test_scores]\n",
    "#     map50_values = [item[1] for item in sorted_test_scores]\n",
    "\n",
    "#     plt.plot(test_dataset_sizes, map50_values, marker='o', linestyle='-',\n",
    "#              label=f\"{model_shot_count}-shot Model\")\n",
    "\n",
    "# plt.legend(title=\"Model Dilatih (Shot)\", loc='lower right')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Optional: Print the full results in a table-like format\n",
    "# print(\"\\n--- Complete mAP@0.5 Results ---\")\n",
    "# for model_shot_count, scores_by_dataset in all_map50_scores.items():\n",
    "#     print(f\"Model {model_shot_count}-shot:\")\n",
    "#     for test_img_count, score in scores_by_dataset.items():\n",
    "#         print(f\"  - Tested on {test_img_count}-images: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f370280",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/support_set\", \"dataset/support_set\")\n",
    "    copy_to_google_drive(\"results/support_set\", \"results/support_set\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
