{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42aadcc9",
   "metadata": {},
   "source": [
    "# Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b155fc2",
   "metadata": {},
   "source": [
    "## Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685e7c1",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.data.annotator import auto_annotate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf4078",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Object Detection\n",
    "rf = Roboflow(api_key=\"MQdx0fMQ8FiQPaS1VHRH\")\n",
    "project = rf.workspace(\"abiya-thesis\").project(\"plant-pathology-2021-object-detection-j1jvh\")\n",
    "version = project.version(9)\n",
    "dataset = version.download(\"yolov11\", location=\"dataset/object_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Mask Lesi\n",
    "rf = Roboflow(api_key=\"MQdx0fMQ8FiQPaS1VHRH\")\n",
    "project = rf.workspace(\"abiya-thesis\").project(\"plant-pathology-2021-instance-segmentation-h5pim\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"yolov11\", location=\"dataset/mask_lesi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3821de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Mask Lesi and Leaf\n",
    "rf = Roboflow(api_key=\"MQdx0fMQ8FiQPaS1VHRH\")\n",
    "project = rf.workspace(\"abiya-thesis\").project(\"plant-pathology-2021-instance-segmentation-h5pim\")\n",
    "version = project.version(5)\n",
    "dataset = version.download(\"yolov11\", location=\"dataset/mask_lesi_and_leaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acae910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Leaf Class Only\n",
    "rf = Roboflow(api_key=\"MQdx0fMQ8FiQPaS1VHRH\")\n",
    "project = rf.workspace(\"abiya-thesis\").project(\"plant-pathology-2021-object-detection-j1jvh\")\n",
    "version = project.version(11)\n",
    "dataset = version.download(\"yolov11\", location=\"dataset/object_detection_leaf_class_only\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903a1d6",
   "metadata": {},
   "source": [
    "## Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dba498",
   "metadata": {},
   "outputs": [],
   "source": [
    "GoogleDrive = False\n",
    "if GoogleDrive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    def copy_to_google_drive(source_folder, destination_folder):\n",
    "        !cp -r /content/{source_folder} /content/gdrive/MyDrive/{destination_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8457ccb",
   "metadata": {},
   "source": [
    "## Global Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_and_save_time(start_time, output_path):\n",
    "    elapsed = time.time() - start_time\n",
    "    h, rem = divmod(elapsed, 3600)\n",
    "    m, s = divmod(rem, 60)\n",
    "    formatted = f\"{int(h)}h {int(m)}m {int(s)}s\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed257141",
   "metadata": {},
   "source": [
    "## Training Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = ['yolo11n.pt', 'yolo11s.pt', 'yolo11m.pt']\n",
    "sizes = ['nano', 'small', 'medium']\n",
    "epochs = 100\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_objectDetection = \"dataset/object_detection\"\n",
    "project_base_objectDetection = \"results/object_detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, size in zip(models, sizes):\n",
    "    print(f\"Training {size} model...\")\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    start_time = time.time()\n",
    "    training = model.train(\n",
    "        data=f\"{data_path_objectDetection}/data.yaml\", \n",
    "        epochs=epochs, \n",
    "        imgsz=640, \n",
    "        batch=batch, \n",
    "        project=f\"{project_base_objectDetection}/training\", \n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True\n",
    "    )\n",
    "\n",
    "    measure_and_save_time(start_time, f\"{project_base_objectDetection}/training/summary/time/{size}-train-time.txt\")\n",
    "    csv_filename = f\"{project_base_objectDetection}/training/summary/{size}-training-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(training.to_csv())\n",
    "\n",
    "    validation = model.val(\n",
    "        data=f\"{data_path_objectDetection}/data.yaml\", \n",
    "        imgsz=640,\n",
    "        project=f\"{project_base_objectDetection}/validation\",\n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True,\n",
    "        split=\"test\"\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_objectDetection}/validation/summary/{size}-validation-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(validation.to_csv())\n",
    "\n",
    "    print(f\"Finished training and validating {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    print(f\"Running prediction for {size} model...\")\n",
    "\n",
    "    model = YOLO(f\"results/object_detection/training/{size}/weights/best.pt\")\n",
    "\n",
    "    # Define the output directory for the current model size\n",
    "    output_dir = f\"results/object_detection/predict/{size}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = model(\n",
    "        source=f\"{data_path_objectDetection}/test/images\", \n",
    "        exist_ok=True,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        original_filename = os.path.basename(result.path)\n",
    "        save_path = os.path.join(output_dir, original_filename)\n",
    "        result.save(filename=save_path)\n",
    "\n",
    "    print(f\"Finished predicting for {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930929bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for size in sizes:\n",
    "    df = pd.read_csv(f\"{project_base_objectDetection}/training/{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - Model {size}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_objectDetection}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/object_detection\", \"dataset/object_detection\")\n",
    "    copy_to_google_drive(\"results/object_detection\", \"results/object_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12cf94",
   "metadata": {},
   "source": [
    "## Semi Auto Annotate Mask Daun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e26998",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"train\",\"valid\",'test']\n",
    "sam2_model = \"sam2.1_b.pt\"\n",
    "data_path_maskDaun = \"dataset/mask_daun\"\n",
    "project_base_maskDaun = \"results/mask_daun\"\n",
    "best_objectDetection_model_size = \"medium\"\n",
    "best_objectDetection_model_path = f\"{project_base_objectDetection}/training/{best_objectDetection_model_size}/weights/best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ea408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pindahkan original images ke mask daun\n",
    "for split in splits:\n",
    "    shutil.copytree(src=f\"{data_path_objectDetection}/{split}/images\", dst=f\"{data_path_maskDaun}/{split}/images\", dirs_exist_ok=True)\n",
    "\n",
    "shutil.copy(src=f\"{data_path_objectDetection}/data.yaml\", dst=f\"{data_path_maskDaun}/data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba7b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    auto_annotate(data=f\"{data_path_maskDaun}/{split}/images/\", det_model=best_objectDetection_model_path, sam_model=sam2_model, output_dir=f\"{data_path_maskDaun}/{split}/labels/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5a7fa",
   "metadata": {},
   "source": [
    "## Training Mask Daun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f541595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = ['yolo11n-seg.pt', 'yolo11s-seg.pt', 'yolo11m-seg.pt']\n",
    "sizes = ['nano', 'small', 'medium']\n",
    "epochs = 100\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_maskDaun = \"dataset/mask_daun\"\n",
    "project_base_maskDaun = \"results/mask_daun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, size in zip(models, sizes):\n",
    "    print(f\"Training {size} model...\")\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    start_time = time.time()\n",
    "    training = model.train(\n",
    "        data=f\"{data_path_maskDaun}/data.yaml\", \n",
    "        epochs=epochs, \n",
    "        imgsz=640, \n",
    "        batch=batch, \n",
    "        project=f\"{project_base_maskDaun}/training\", \n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True\n",
    "    )\n",
    "\n",
    "    measure_and_save_time(start_time, f\"{project_base_maskDaun}/training/summary/time/{size}-train-time.txt\")\n",
    "    csv_filename = f\"{project_base_maskDaun}/training/summary/{size}-training-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(training.to_csv())\n",
    "\n",
    "    validation = model.val(\n",
    "        data=f\"{data_path_maskDaun}/data.yaml\", \n",
    "        imgsz=640,\n",
    "        project=f\"{project_base_maskDaun}/validation\",\n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True,\n",
    "        split=\"test\"\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_maskDaun}/validation/summary/{size}-validation-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(validation.to_csv())\n",
    "\n",
    "    print(f\"Finished training and validating {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b01225",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    print(f\"Running prediction for {size} model...\")\n",
    "\n",
    "    model = YOLO(f\"results/mask_daun/training/{size}/weights/best.pt\")\n",
    "\n",
    "    # Define the output directory for the current model size\n",
    "    output_dir = f\"results/mask_daun/predict/{size}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = model(\n",
    "        source=f\"{data_path_maskDaun}/test/images\", \n",
    "        exist_ok=True,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        original_filename = os.path.basename(result.path)\n",
    "        save_path = os.path.join(output_dir, original_filename)\n",
    "        result.save(filename=save_path)\n",
    "\n",
    "    print(f\"Finished predicting for {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99138daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for size in sizes:\n",
    "    df = pd.read_csv(f\"{project_base_maskDaun}/training/{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - Model {size}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_maskDaun}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005aad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/mask_daun\", \"dataset/mask_daun\")\n",
    "    copy_to_google_drive(\"results/mask_daun\", \"results/mask_daun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d607f9",
   "metadata": {},
   "source": [
    "## Training Mask Lesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = ['yolo11n-seg.pt', 'yolo11s-seg.pt', 'yolo11m-seg.pt']\n",
    "sizes = ['nano', 'small', 'medium']\n",
    "epochs = 100\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_maskLesi = \"dataset/mask_lesi\"\n",
    "project_base_maskLesi = \"results/mask_lesi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, size in zip(models, sizes):\n",
    "    print(f\"Training {size} model...\")\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    start_time = time.time()\n",
    "    training = model.train(\n",
    "        data=f\"{data_path_maskLesi}/data.yaml\", \n",
    "        epochs=epochs, \n",
    "        imgsz=640, \n",
    "        batch=batch, \n",
    "        project=f\"{project_base_maskLesi}/training\", \n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True\n",
    "    )\n",
    "\n",
    "    measure_and_save_time(start_time, f\"{project_base_maskLesi}/training/summary/time/{size}-train-time.txt\")\n",
    "    csv_filename = f\"{project_base_maskLesi}/training/summary/{size}-training-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(training.to_csv())\n",
    "\n",
    "    validation = model.val(\n",
    "        data=f\"{data_path_maskLesi}/data.yaml\", \n",
    "        imgsz=640,\n",
    "        project=f\"{project_base_maskLesi}/validation\",\n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True,\n",
    "        split=\"test\"\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_maskLesi}/validation/summary/{size}-validation-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(validation.to_csv())\n",
    "\n",
    "    print(f\"Finished training and validating {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de773ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    print(f\"Running prediction for {size} model...\")\n",
    "\n",
    "    model = YOLO(f\"results/mask_lesi/training/{size}/weights/best.pt\")\n",
    "\n",
    "    # Define the output directory for the current model size\n",
    "    output_dir = f\"results/mask_lesi/predict/{size}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = model(\n",
    "        source=f\"{data_path_maskLesi}/test/images\", # Kedepannya ubah menjadi /test jika sudah ada test yang di anotasi\n",
    "        exist_ok=True,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        original_filename = os.path.basename(result.path)\n",
    "        save_path = os.path.join(output_dir, original_filename)\n",
    "        result.save(filename=save_path)\n",
    "\n",
    "    print(f\"Finished predicting for {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for size in sizes:\n",
    "    df = pd.read_csv(f\"{project_base_maskLesi}/training/{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - Model {size}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_maskLesi}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02fb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/mask_lesi\", \"dataset/mask_lesi\")\n",
    "    copy_to_google_drive(\"results/mask_lesi\", \"results/mask_lesi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b33735b",
   "metadata": {},
   "source": [
    "## Training Mask Lesi dan Daun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = ['yolo11n-seg.pt', 'yolo11s-seg.pt', 'yolo11m-seg.pt']\n",
    "sizes = ['nano', 'small', 'medium']\n",
    "epochs = 100\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_maskLesi_and_maskLeaf = \"dataset/mask_lesi_and_leaf\"\n",
    "project_base_maskLesi_and_maskLeaf = \"results/mask_lesi_and_leaf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, size in zip(models, sizes):\n",
    "    print(f\"Training {size} model...\")\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    start_time = time.time()\n",
    "    training = model.train(\n",
    "        data=f\"{data_path_maskLesi_and_maskLeaf}/data.yaml\", \n",
    "        epochs=epochs, \n",
    "        imgsz=640, \n",
    "        batch=batch, \n",
    "        project=f\"{project_base_maskLesi_and_maskLeaf}/training\", \n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True\n",
    "    )\n",
    "\n",
    "    measure_and_save_time(start_time, f\"{project_base_maskLesi_and_maskLeaf}/training/summary/time/{size}-train-time.txt\")\n",
    "    csv_filename = f\"{project_base_maskLesi_and_maskLeaf}/training/summary/{size}-training-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(training.to_csv())\n",
    "\n",
    "    validation = model.val(\n",
    "        data=f\"{data_path_maskLesi_and_maskLeaf}/data.yaml\", \n",
    "        imgsz=640,\n",
    "        project=f\"{project_base_maskLesi}/validation\",\n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True,\n",
    "        split=\"test\"\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_maskLesi}/validation/summary/{size}-validation-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(validation.to_csv())\n",
    "\n",
    "    print(f\"Finished training and validating {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    print(f\"Running prediction for {size} model...\")\n",
    "\n",
    "    model = YOLO(f\"results/mask_lesi/training/{size}/weights/best.pt\")\n",
    "\n",
    "    # Define the output directory for the current model size\n",
    "    output_dir = f\"results/mask_lesi/predict/{size}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = model(\n",
    "        source=f\"{data_path_maskLesi_and_maskLeaf}/test/images\",\n",
    "        exist_ok=True,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        original_filename = os.path.basename(result.path)\n",
    "        save_path = os.path.join(output_dir, original_filename)\n",
    "        result.save(filename=save_path)\n",
    "\n",
    "    print(f\"Finished predicting for {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f47165",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for size in sizes:\n",
    "    df = pd.read_csv(f\"{project_base_maskLesi_and_maskLeaf}/training/{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - Model {size}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_maskLesi_and_maskLeaf}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/mask_lesi_and_leaf\", \"dataset/mask_lesi_and_leaf\")\n",
    "    copy_to_google_drive(\"results/mask_lesi_and_leaf\", \"results/mask_lesi_and_leaf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876eb91e",
   "metadata": {},
   "source": [
    "## Object Detection Class Leaf Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = ['yolo11n.pt', 'yolo11s.pt', 'yolo11m.pt']\n",
    "sizes = ['nano', 'small', 'medium']\n",
    "epochs = 100\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_objectDetection_leaf_class_only = \"dataset/object_detection_leaf_class_only\"\n",
    "project_base_objectDetection_leaf_class_only = \"results/object_detection_leaf_class_only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, size in zip(models, sizes):\n",
    "    print(f\"Training {size} model...\")\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    training = model.train(\n",
    "        data=f\"{data_path_objectDetection_leaf_class_only}/data.yaml\", \n",
    "        epochs=epochs, \n",
    "        imgsz=640, \n",
    "        batch=batch, \n",
    "        project=f\"{project_base_objectDetection_leaf_class_only}/training\", \n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_objectDetection_leaf_class_only}/training/summary/{size}-training-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(training.to_csv())\n",
    "\n",
    "    validation = model.val(\n",
    "        data=f\"{data_path_objectDetection_leaf_class_only}/data.yaml\", \n",
    "        imgsz=640,\n",
    "        project=f\"{project_base_objectDetection_leaf_class_only}/validation\",\n",
    "        name=f\"{size}\",\n",
    "        exist_ok=True,\n",
    "        split=\"test\"\n",
    "    )\n",
    "\n",
    "    csv_filename = f\"{project_base_objectDetection_leaf_class_only}/validation/summary/{size}-validation-metrics.csv\"\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, \"w\") as f:\n",
    "        f.write(validation.to_csv())\n",
    "\n",
    "    print(f\"Finished training and validating {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    print(f\"Running prediction for {size} model...\")\n",
    "\n",
    "    model = YOLO(f\"results/object_detection_leaf_class_only/training/{size}/weights/best.pt\")\n",
    "\n",
    "    # Define the output directory for the current model size\n",
    "    output_dir = f\"results/object_detection_leaf_class_only/predict/{size}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = model(\n",
    "        source=f\"{data_path_objectDetection_leaf_class_only}/test/images\", \n",
    "        exist_ok=True,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        original_filename = os.path.basename(result.path)\n",
    "        save_path = os.path.join(output_dir, original_filename)\n",
    "        result.save(filename=save_path)\n",
    "\n",
    "    print(f\"Finished predicting for {size} model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f605f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for size in sizes:\n",
    "    df = pd.read_csv(f\"{project_base_objectDetection_leaf_class_only}/training/{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - Model {size}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_objectDetection_leaf_class_only}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4235f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/object_detection_leaf_class_only\", \"dataset/object_detection_leaf_class_only\")\n",
    "    copy_to_google_drive(\"results/object_detection_leaf_class_only\", \"results/object_detection_leaf_class_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b50cb",
   "metadata": {},
   "source": [
    "## Fine Tune For Detect Healthy, Rust, and Scab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb41dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path sumber dataset YOLO\n",
    "image_dir = Path(\"dataset/object_detection/train/images\")\n",
    "label_dir = Path(\"dataset/object_detection/train/labels\")\n",
    "\n",
    "# Path target few-shot\n",
    "few_shot_base = Path(\"dataset/few_shot\")\n",
    "\n",
    "# Mapping class ID ke nama\n",
    "class_map = {0: \"frog-eye-leaf-spot\", 1: \"healthy\", 2: \"rust\"}\n",
    "\n",
    "# Buat index gambar per class berdasarkan nama file\n",
    "class_to_files = defaultdict(list)\n",
    "for img_file in sorted(image_dir.glob(\"*.jpg\")):\n",
    "    filename_lower = img_file.name.lower()\n",
    "    if \"frog-eye-leaf-spot\" in filename_lower:\n",
    "        cls = 0\n",
    "    elif \"healthy\" in filename_lower:\n",
    "        cls = 1\n",
    "    elif \"rust\" in filename_lower:\n",
    "        cls = 2\n",
    "    else:\n",
    "        continue  # Skip jika tidak dikenali\n",
    "\n",
    "    label_file = label_dir / (img_file.stem + \".txt\")\n",
    "    if label_file.exists():\n",
    "        class_to_files[cls].append((img_file, label_file))\n",
    "\n",
    "# Generate few-shot data\n",
    "for n in [5] + list(range(10, 51, 5)):\n",
    "    target_image_dir = few_shot_base / f\"{n}-shot\" / \"images\"\n",
    "    target_label_dir = few_shot_base / f\"{n}-shot\" / \"labels\"\n",
    "    target_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "    target_label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for cls in class_map:\n",
    "        files = class_to_files[cls][:n]  # ambil berurutan\n",
    "        for img_path, label_path in files:\n",
    "            shutil.copy(img_path, target_image_dir / img_path.name)\n",
    "            shutil.copy(label_path, target_label_dir / label_path.name)\n",
    "\n",
    "# Copy data.yaml\n",
    "shutil.copy(\"dataset/object_detection/data.yaml\", \"dataset/few_shot/data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabel Global\n",
    "models = {\"nano\": \"results/object_detection_leaf_class_only/training/nano/weights/best.pt\",\n",
    "          \"small\": \"results/object_detection_leaf_class_only/training/small/weights/best.pt\",\n",
    "          \"medium\": \"results/object_detection_leaf_class_only/training/medium/weights/best.pt\"}\n",
    "sizes = [\"nano\", \"small\", \"medium\"]\n",
    "shots = [5] + list(range(10, 51, 5))\n",
    "size_to_shots = {\n",
    "    \"nano\":   shots,\n",
    "    \"small\":  shots,\n",
    "    \"medium\": shots\n",
    "}\n",
    "epochs = 50\n",
    "batch = -1 # atau -1 untuk limitasi gpu 60%\n",
    "data_path_few_shot = \"dataset/few_shot\"\n",
    "project_base_few_shot = \"results/few_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    for shot in size_to_shots[size]:\n",
    "        # 1. Load original YAML\n",
    "        data_yaml_path = f\"{data_path_few_shot}/data.yaml\"\n",
    "        with open(data_yaml_path, 'r') as f:\n",
    "            data_yaml = yaml.safe_load(f)\n",
    "\n",
    "        # 2. Ubah path train sesuai shot\n",
    "        data_yaml['train'] = f\"../{shot}-shot/images\"\n",
    "\n",
    "        # 3. Simpan YAML baru ke file sementara\n",
    "        temp_yaml_path = f\"{data_path_few_shot}/data-shot.yaml\"\n",
    "        with open(temp_yaml_path, 'w') as f:\n",
    "            yaml.dump(data_yaml, f, sort_keys=False, default_flow_style=True)\n",
    "\n",
    "        model = YOLO(models[size])\n",
    "        start_time = time.time()\n",
    "        training = model.train(\n",
    "            data = temp_yaml_path,\n",
    "            epochs = epochs,\n",
    "            imgsz=640,\n",
    "            batch=batch,\n",
    "            project=f\"{project_base_few_shot}/training\",\n",
    "            name=f\"{shot}-shot-{size}\",\n",
    "            exist_ok=True,\n",
    "            # patience=25,\n",
    "            # freeze=8\n",
    "        )\n",
    "\n",
    "        csv_filename = f\"{project_base_few_shot}/training/summary/{shot}-shot-{size}-training-metrics.csv\"\n",
    "        os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "        with open(csv_filename, \"w\") as f:\n",
    "            f.write(training.to_csv())\n",
    "\n",
    "        validation = model.val(\n",
    "            data = temp_yaml_path,\n",
    "            imgsz=640,\n",
    "            project=f\"{project_base_few_shot}/validation\",\n",
    "            name=f\"{shot}-shot-{size}\",\n",
    "            exist_ok=True,\n",
    "            split=\"test\"\n",
    "        )\n",
    "\n",
    "        csv_filename = f\"{project_base_few_shot}/validation/summary/{shot}-shot-{size}-validation-metrics.csv\"\n",
    "        os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "        with open(csv_filename, \"w\") as f:\n",
    "            f.write(validation.to_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for shot in shots:\n",
    "    df = pd.read_csv(f\"{project_base_few_shot}/training/{shot}-shot-{size}/results.csv\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot mAP50 dan mAP50-95\n",
    "    plt.plot(df.index, df[\"metrics/mAP50(B)\"], label=\"mAP@0.5\")\n",
    "    plt.plot(df.index, df[\"metrics/mAP50-95(B)\"], label=\"mAP@0.5:0.95\")\n",
    "\n",
    "    # Tambahkan label dan judul\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.title(f\"Model Performance - {size} | {shot}-shot\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan plot\n",
    "    graph_output_dir = f\"{project_base_few_shot}/graph\"\n",
    "    os.makedirs(graph_output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(graph_output_dir, f\"mAP_comparison_{shot}-shot-{size}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33725b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/few_shot\", \"dataset/few_shot\")\n",
    "    copy_to_google_drive(\"results/few_shot\", \"results/few_shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8bc06",
   "metadata": {},
   "source": [
    "## Perbandingan Beberapa k-query Set vs n-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeadb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path sumber dataset YOLO\n",
    "image_dir = Path(\"dataset/object_detection/train/images\")\n",
    "label_dir = Path(\"dataset/object_detection/train/labels\")\n",
    "\n",
    "# Path target support-set\n",
    "support_set_base = Path(\"dataset/support_set\")\n",
    "\n",
    "# Mapping class ID ke nama\n",
    "class_map = {0: \"frog-eye-leaf-spot\", 1: \"healthy\", 2: \"rust\"}\n",
    "\n",
    "# Buat index gambar per class berdasarkan nama file\n",
    "class_to_files = defaultdict(list)\n",
    "for img_file in sorted(image_dir.glob(\"*.jpg\")):\n",
    "    filename_lower = img_file.name.lower()\n",
    "    if \"frog-eye-leaf-spot\" in filename_lower:\n",
    "        cls = 0\n",
    "    elif \"healthy\" in filename_lower:\n",
    "        cls = 1\n",
    "    elif \"rust\" in filename_lower:\n",
    "        cls = 2\n",
    "    else:\n",
    "        continue  # Skip jika tidak dikenali\n",
    "\n",
    "    label_file = label_dir / (img_file.stem + \".txt\")\n",
    "    if label_file.exists():\n",
    "        class_to_files[cls].append((img_file, label_file))\n",
    "\n",
    "# Generate support-set data\n",
    "for n in [5] + list(range(10, 51, 5)):\n",
    "    target_image_dir = support_set_base / f\"{n}-images\" / \"images\"\n",
    "    target_label_dir = support_set_base / f\"{n}-images\" / \"labels\"\n",
    "    target_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "    target_label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for cls in class_map:\n",
    "        files = class_to_files[cls][-n:]  # ambil n gambar terakhir\n",
    "        for img_path, label_path in files:\n",
    "            shutil.copy(img_path, target_image_dir / img_path.name)\n",
    "            shutil.copy(label_path, target_label_dir / label_path.name)\n",
    "\n",
    "# Copy data.yaml\n",
    "shutil.copy(\"dataset/object_detection/data.yaml\", \"dataset/support_set/data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Variabel Global\n",
    "# ======================================================\n",
    "sizes = [\"nano\", \"small\", \"medium\"]\n",
    "shots = [5] + list(range(10, 51, 5))\n",
    "project_base_few_shot = \"results/few_shot\"\n",
    "support_set_base = \"dataset/support_set\"\n",
    "\n",
    "# Path YAML original\n",
    "data_yaml_original_path = f\"{support_set_base}/data.yaml\"\n",
    "temp_data_yaml_dir = os.path.dirname(data_yaml_original_path)\n",
    "os.makedirs(temp_data_yaml_dir, exist_ok=True)\n",
    "\n",
    "# Folder summary\n",
    "summary_dir = f\"{project_base_few_shot}/validation/summary\"\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "\n",
    "# ======================================================\n",
    "# Dictionary hasil evaluasi\n",
    "# all_map50_scores[size][model_shot][test_images] = mAP\n",
    "# ======================================================\n",
    "all_map50_scores = {size: {model_shot: {} for model_shot in shots} for size in sizes}\n",
    "\n",
    "# ======================================================\n",
    "# Loop evaluasi\n",
    "# ======================================================\n",
    "for size in sizes:\n",
    "    for model_shot in shots:\n",
    "        print(f\"\\n--- Evaluasi model {size} ({model_shot}-shot) ---\")\n",
    "        model_path = f\"{project_base_few_shot}/training/{model_shot}-shot-{size}/weights/best.pt\"\n",
    "        model = YOLO(model_path)\n",
    "\n",
    "        for test_images in shots:\n",
    "            # 1. Load yaml asli\n",
    "            with open(data_yaml_original_path, 'r') as f:\n",
    "                data_yaml = yaml.safe_load(f)\n",
    "\n",
    "            # 2. Set test/val/train ke support-set test_images\n",
    "            data_yaml['test']  = f\"../{test_images}-images/images\"\n",
    "            data_yaml['val']   = f\"../{test_images}-images/images\"\n",
    "            data_yaml['train'] = f\"../{test_images}-images/images\"\n",
    "\n",
    "            # 3. Simpan yaml sementara\n",
    "            temp_yaml_path = os.path.join(\n",
    "                temp_data_yaml_dir,\n",
    "                f\"eval_{model_shot}-shot-{size}_on_{test_images}-images.yaml\"\n",
    "            )\n",
    "            with open(temp_yaml_path, 'w') as f:\n",
    "                yaml.dump(data_yaml, f, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "            # 4. Validasi\n",
    "            results = model.val(\n",
    "                data=temp_yaml_path,\n",
    "                split=\"test\",\n",
    "                imgsz=640,\n",
    "                project=f\"{project_base_few_shot}/validation\",\n",
    "                name=f\"{model_shot}-shot-{size}-on-{test_images}-images\",\n",
    "                exist_ok=True\n",
    "            )\n",
    "            map50 = results.results_dict.get(\"metrics/mAP50(B)\", 0.0)\n",
    "            all_map50_scores[size][model_shot][test_images] = map50\n",
    "\n",
    "            print(f\"Model {model_shot}-shot-{size} diuji di {test_images}-images â†’ mAP50 = {map50:.4f}\")\n",
    "\n",
    "        # Hapus folder runs agar tidak menumpuk\n",
    "        if os.path.exists(\"runs\"):\n",
    "            shutil.rmtree(\"runs\")\n",
    "\n",
    "# ======================================================\n",
    "# Simpan hasil ke CSV summary per size\n",
    "# ======================================================\n",
    "for size in sizes:\n",
    "    df = pd.DataFrame(all_map50_scores[size]).T  # model_shot sebagai index, test_images sebagai kolom\n",
    "    csv_path = os.path.join(summary_dir, f\"{size}-results.csv\")\n",
    "    df.to_csv(csv_path, index=True)\n",
    "    print(f\"Hasil ringkasan {size} disimpan di {csv_path}\")\n",
    "\n",
    "# ======================================================\n",
    "# Visualisasi hasil (contoh: plot semua size)\n",
    "# ======================================================\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title(\"Perbandingan mAP@0.5 Few-Shot (Semua Model)\")\n",
    "plt.xlabel(\"Jumlah Gambar per Kelas (Support-Set)\")\n",
    "plt.ylabel(\"mAP@0.5\")\n",
    "plt.xticks(shots)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "for size in sizes:\n",
    "    for model_shot in shots:\n",
    "        sorted_scores = sorted(all_map50_scores[size][model_shot].items())\n",
    "        x = [item[0] for item in sorted_scores]\n",
    "        y = [item[1] for item in sorted_scores]\n",
    "        plt.plot(x, y, marker='o', linestyle='-',\n",
    "                 label=f\"{size} {model_shot}-shot\")\n",
    "\n",
    "plt.legend(title=\"Model (size & shot)\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ======================================================\n",
    "# Cetak hasil ke console\n",
    "# ======================================================\n",
    "print(\"\\n--- Hasil Lengkap mAP@0.5 ---\")\n",
    "for size in sizes:\n",
    "    print(f\"\\nModel {size}:\")\n",
    "    for model_shot, scores_by_test in all_map50_scores[size].items():\n",
    "        for test_img_count, score in scores_by_test.items():\n",
    "            print(f\"  - {model_shot}-shot diuji di {test_img_count}-images: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "# import matplotlib.pyplot as plt\n",
    "# from ultralytics import YOLO\n",
    "# import os\n",
    "# import shutil # Import shutil untuk operasi penghapusan folder\n",
    "\n",
    "# # Define the models to be evaluated and the test datasets\n",
    "# # Models trained with these 'shot' counts\n",
    "# lists_model_shots = [5] + list(range(10, 51, 5))\n",
    "# # Test datasets with these 'image' counts per class\n",
    "# lists_test_dataset_images = [5] + list(range(10, 51, 5))\n",
    "\n",
    "# # Dictionary to store mAP@0.5 scores for each model-test_dataset combination\n",
    "# # Format: {model_shot_count: {test_dataset_image_count: mAP50_score}}\n",
    "# all_map50_scores = {}\n",
    "\n",
    "# # Path to your original data.yaml file\n",
    "# data_yaml_original_path = \"dataset/support_set/data.yaml\"\n",
    "# # **MODIFIKASI DI SINI:** Temporary directory to save modified data.yaml files\n",
    "# # Sekarang akan disimpan di direktori yang sama dengan data_yaml_original_path\n",
    "# temp_data_yaml_dir = os.path.dirname(data_yaml_original_path)\n",
    "# # Default base directory for Ultralytics YOLO runs\n",
    "# yolo_runs_dir = \"runs\" # This is the default folder where YOLO saves validation results\n",
    "\n",
    "# # Ensure the temporary directory for YAMLs exists (though it should for support_set)\n",
    "# os.makedirs(temp_data_yaml_dir, exist_ok=True)\n",
    "\n",
    "# # Function to clear the runs/detect folder\n",
    "# def clear_yolo_runs_directory(base_dir=\".\"):\n",
    "#     \"\"\"\n",
    "#     Menghapus folder 'runs/detect' dari direktori dasar yang diberikan.\n",
    "#     \"\"\"\n",
    "#     detect_run_path = os.path.join(base_dir, yolo_runs_dir, 'detect')\n",
    "#     if os.path.exists(detect_run_path):\n",
    "#         print(f\"Menghapus direktori: {detect_run_path}\")\n",
    "#         try:\n",
    "#             shutil.rmtree(detect_run_path)\n",
    "#             print(f\"Direktori {detect_run_path} berhasil dihapus.\")\n",
    "#         except OSError as e:\n",
    "#             print(f\"Error: Gagal menghapus direktori {detect_run_path}. {e}\")\n",
    "#     else:\n",
    "#         print(f\"Direktori {detect_run_path} tidak ditemukan, tidak perlu dihapus.\")\n",
    "\n",
    "# # --- Start of Evaluation Loop ---\n",
    "# # Loop through each trained model\n",
    "# for model_shot_count in lists_model_shots:\n",
    "#     print(f\"\\n--- Testing Model: {model_shot_count}-shot ---\")\n",
    "\n",
    "#     # Clear YOLO runs directory before validating each model\n",
    "#     # This ensures a clean slate for each model's validation across test sets\n",
    "#     clear_yolo_runs_directory()\n",
    "\n",
    "#     # Initialize a dictionary to store scores for the current model\n",
    "#     all_map50_scores[model_shot_count] = {}\n",
    "\n",
    "#     # # Load the trained model\n",
    "#     model_path = f\"results/few_shot/training/{model_shot_count}-shot/weights/best.pt\"\n",
    "#     # try:\n",
    "#     #     model = YOLO(model_path)\n",
    "#     # except Exception as e:\n",
    "#     #     print(f\"Error loading model {model_path}: {e}. Skipping this model.\")\n",
    "#     #     continue # Skip to the next model if loading fails\n",
    "\n",
    "#     # Loop through each test dataset\n",
    "#     for test_image_count in lists_test_dataset_images:\n",
    "#         model = YOLO(model_path)\n",
    "#         # 1. Load the original data.yaml\n",
    "#         with open(data_yaml_original_path, 'r') as f:\n",
    "#             data_yaml = yaml.safe_load(f)\n",
    "\n",
    "#         # 2. Update the 'test' path to the current test dataset\n",
    "#         # This path is relative to the data.yaml itself.\n",
    "#         # Ensure that 'dataset/support_set' is the base for 'X-images'\n",
    "#         # For example, if data.yaml is in 'dataset/support_set/',\n",
    "#         # and images are in 'dataset/support_set/5-images/',\n",
    "#         # then the relative path from data.yaml to 5-images is '../5-images/images'\n",
    "#         # This relative path needs to be correct based on your actual file structure.\n",
    "#         data_yaml['test'] = f\"../{test_image_count}-images/images\"\n",
    "#         data_yaml['train'] = f\"../{test_image_count}-images/images\"\n",
    "#         data_yaml['val'] = f\"../{test_image_count}-images/images\"\n",
    "\n",
    "\n",
    "#         # 3. Save the modified YAML to a temporary file in the specified directory\n",
    "#         # **MODIFIKASI DI SINI:** temp_yaml_path sekarang menunjuk ke data_yaml_original_path's directory\n",
    "#         temp_yaml_filename = f\"data_support_test_{model_shot_count}model_{test_image_count}.yaml\"\n",
    "#         temp_yaml_path = os.path.join(temp_data_yaml_dir, temp_yaml_filename)\n",
    "#         with open(temp_yaml_path, 'w') as f:\n",
    "#             yaml.dump(data_yaml, f, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "#         # 4. Perform model validation\n",
    "#         print(f\"  Validating {model_shot_count}-shot model on {test_image_count}-images dataset...\")\n",
    "#         try:\n",
    "#             # Pass the path to the newly created temporary YAML file\n",
    "#             print(\"temp_yaml_path\",temp_yaml_path)\n",
    "#             results = model.val(data=temp_yaml_path)\n",
    "#             # Get mAP@0.5 score\n",
    "#             map50 = results.results_dict.get(\"metrics/mAP50(B)\", 0.0)\n",
    "#             all_map50_scores[model_shot_count][test_image_count] = map50\n",
    "#             print(f\"    mAP@0.5: {map50:.4f}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error validating {model_shot_count}-shot model on {test_image_count}-images: {e}\")\n",
    "#             all_map50_scores[model_shot_count][test_image_count] = 0.0 # Set to 0 if an error occurs\n",
    "\n",
    "#         # Optional: Remove the temporary YAML file after use\n",
    "#         # os.rmdir(\"runs/\")\n",
    "#         shutil.rmtree(\"runs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# # --- Visualization of Results ---\n",
    "# plt.figure(figsize=(15, 8))\n",
    "# plt.title(\"Perbandingan mAP@0.5 Model Few-Shot pada Berbagai Ukuran Dataset Pengujian\")\n",
    "# plt.xlabel(\"Ukuran Dataset Pengujian (Jumlah Gambar per Kelas)\")\n",
    "# plt.ylabel(\"mAP@0.5\")\n",
    "# plt.xticks(lists_test_dataset_images)\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# # Plot a line for each trained model\n",
    "# for model_shot_count, test_scores_dict in all_map50_scores.items():\n",
    "#     # Sort the test scores by dataset size for proper plotting\n",
    "#     sorted_test_scores = sorted(test_scores_dict.items())\n",
    "\n",
    "#     # Extract test dataset image counts and corresponding mAP50 scores\n",
    "#     test_dataset_sizes = [item[0] for item in sorted_test_scores]\n",
    "#     map50_values = [item[1] for item in sorted_test_scores]\n",
    "\n",
    "#     plt.plot(test_dataset_sizes, map50_values, marker='o', linestyle='-',\n",
    "#              label=f\"{model_shot_count}-shot Model\")\n",
    "\n",
    "# plt.legend(title=\"Model Dilatih (Shot)\", loc='lower right')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Optional: Print the full results in a table-like format\n",
    "# print(\"\\n--- Complete mAP@0.5 Results ---\")\n",
    "# for model_shot_count, scores_by_dataset in all_map50_scores.items():\n",
    "#     print(f\"Model {model_shot_count}-shot:\")\n",
    "#     for test_img_count, score in scores_by_dataset.items():\n",
    "#         print(f\"  - Tested on {test_img_count}-images: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f370280",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GoogleDrive:\n",
    "    copy_to_google_drive(\"dataset/support_set\", \"dataset/support_set\")\n",
    "    copy_to_google_drive(\"results/support_set\", \"results/support_set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a0ebc",
   "metadata": {},
   "source": [
    "## Severity Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07362147",
   "metadata": {},
   "source": [
    "### SINGLE-STAGE (tanpa deteksi) â€” ambil SATU daun terbesar dari mask segmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# SINGLE-STAGE Â· LEAF TERBESAR\n",
    "# ==========================\n",
    "from ultralytics import YOLO\n",
    "import numpy as np, cv2, os, glob, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CFG (ISI SENDIRI) ----------\n",
    "SEG_MODEL_PATH   = \"PATH/TO/yolo11?-seg-best.pt\"  # model seg dengan 5 kelas berikut\n",
    "INPUT_DIR        = \"PATH/TO/images\"\n",
    "OUTPUT_DIR       = \"PATH/TO/output_single_largest\"\n",
    "CSV_PATH         = str(Path(OUTPUT_DIR) / \"summary_single_largest.csv\")\n",
    "CONF_THRESH      = 0.25\n",
    "IOU_THRESH       = 0.5\n",
    "MASK_THRESHOLD   = 0.5\n",
    "SAVE_OVERLAY     = True\n",
    "# ---------------------------------------\n",
    "\n",
    "# Kelas segmentasi (urutannya HARUS sesuai training):\n",
    "# 0: frog-eye-leaf-spot (leaf), 1: frog-eye-leaf-spot_lession,\n",
    "# 2: healthy (leaf), 3: rust (leaf), 4: rust_lession\n",
    "SEG_LEAF_IDS   = [0, 2, 3]\n",
    "PAIR_LESION_ID = {0: 1, 3: 4}  # mapping leaf->lesion; healthy(2) tidak punya lesion\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "model = YOLO(SEG_MODEL_PATH)\n",
    "image_paths = sorted(glob.glob(str(Path(INPUT_DIR) / \"*.*\")))\n",
    "records = []\n",
    "\n",
    "for img_path in image_paths:\n",
    "    img_name = Path(img_path).name\n",
    "    pred = model.predict(source=img_path, conf=CONF_THRESH, iou=IOU_THRESH, verbose=False)[0]\n",
    "\n",
    "    if pred.masks is None or pred.boxes is None or len(pred.masks) == 0:\n",
    "        records.append({\"image\": img_name, \"leaf_class\": None, \"leaf_px\": 0, \"lesion_px\": 0, \"severity_pct\": 0.0})\n",
    "        continue\n",
    "\n",
    "    masks = (pred.masks.data.cpu().numpy() > MASK_THRESHOLD)  # (N,H,W) bool\n",
    "    cls   = pred.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    # Kumpulkan semua leaf-masks beserta class_id-nya\n",
    "    leaf_idxs = [i for i, c in enumerate(cls) if c in SEG_LEAF_IDS]\n",
    "    if not leaf_idxs:\n",
    "        records.append({\"image\": img_name, \"leaf_class\": None, \"leaf_px\": 0, \"lesion_px\": 0, \"severity_pct\": 0.0})\n",
    "        continue\n",
    "\n",
    "    leaf_stack = np.stack([masks[i] for i in leaf_idxs], axis=0)\n",
    "    areas = leaf_stack.reshape(len(leaf_idxs), -1).sum(axis=1)\n",
    "    best_i = leaf_idxs[int(np.argmax(areas))]\n",
    "    leaf_mask = masks[best_i]\n",
    "    leaf_class = cls[best_i]\n",
    "\n",
    "    # Tentukan lesion-mask pasangannya (jika ada)\n",
    "    lesion_mask = np.zeros_like(leaf_mask, dtype=bool)\n",
    "    if leaf_class in PAIR_LESION_ID:\n",
    "        lesion_id = PAIR_LESION_ID[leaf_class]\n",
    "        lesion_idxs = [i for i, c in enumerate(cls) if c == lesion_id]\n",
    "        if lesion_idxs:\n",
    "            lesion_mask = np.any(masks[lesion_idxs], axis=0)\n",
    "\n",
    "    lesion_in_leaf = lesion_mask & leaf_mask\n",
    "    leaf_px   = int(leaf_mask.sum())\n",
    "    lesion_px = int(lesion_in_leaf.sum())\n",
    "    sev = (lesion_px / leaf_px * 100.0) if leaf_px > 0 else 0.0\n",
    "\n",
    "    records.append({\n",
    "        \"image\": img_name,\n",
    "        \"leaf_class\": int(leaf_class),\n",
    "        \"leaf_px\": leaf_px,\n",
    "        \"lesion_px\": lesion_px,\n",
    "        \"severity_pct\": sev\n",
    "    })\n",
    "\n",
    "    if SAVE_OVERLAY:\n",
    "        img = cv2.imread(img_path)\n",
    "        overlay = img.copy()\n",
    "        overlay[leaf_mask]        = (0.7*overlay[leaf_mask] + 0.3*np.array([0,255,0])).astype(np.uint8)\n",
    "        overlay[lesion_in_leaf]   = (0.7*overlay[lesion_in_leaf] + 0.3*np.array([0,0,255])).astype(np.uint8)\n",
    "        cv2.putText(overlay, f\"Severity: {sev:.2f}%\", (10, overlay.shape[0]-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 2, cv2.LINE_AA)\n",
    "        cv2.imwrite(str(Path(OUTPUT_DIR) / f\"{Path(img_name).stem}_overlay.jpg\"), overlay)\n",
    "\n",
    "pd.DataFrame(records).to_csv(CSV_PATH, index=False)\n",
    "print(f\"[Single-stage Â· largest] CSV saved -> {CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3db58",
   "metadata": {},
   "source": [
    "### SINGLE-STAGE (tanpa deteksi) â€” semua daun pada gambar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# SINGLE-STAGE Â· SEMUA DAUN\n",
    "# ==========================\n",
    "from ultralytics import YOLO\n",
    "import numpy as np, cv2, os, glob, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CFG (ISI SENDIRI) ----------\n",
    "SEG_MODEL_PATH   = \"PATH/TO/yolo11?-seg-best.pt\"\n",
    "INPUT_DIR        = \"PATH/TO/images\"\n",
    "OUTPUT_DIR       = \"PATH/TO/output_single_all\"\n",
    "CSV_PATH         = str(Path(OUTPUT_DIR) / \"summary_single_all.csv\")\n",
    "CONF_THRESH      = 0.25\n",
    "IOU_THRESH       = 0.5\n",
    "MASK_THRESHOLD   = 0.5\n",
    "SAVE_OVERLAY     = True\n",
    "# ---------------------------------------\n",
    "\n",
    "SEG_LEAF_IDS   = [0, 2, 3]         # leaf classes\n",
    "PAIR_LESION_ID = {0: 1, 3: 4}      # mapping leaf->lesion\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "model = YOLO(SEG_MODEL_PATH)\n",
    "image_paths = sorted(glob.glob(str(Path(INPUT_DIR) / \"*.*\")))\n",
    "rows, overlay_suffix = [], \"_overlay_all.jpg\"\n",
    "\n",
    "for img_path in image_paths:\n",
    "    img_name = Path(img_path).name\n",
    "    pred = model.predict(source=img_path, conf=CONF_THRESH, iou=IOU_THRESH, verbose=False)[0]\n",
    "\n",
    "    if pred.masks is None or pred.boxes is None or len(pred.masks) == 0:\n",
    "        rows.append({\"image\": img_name, \"leaf_index\": None, \"leaf_class\": None,\n",
    "                     \"leaf_px\": 0, \"lesion_px\": 0, \"severity_pct\": 0.0})\n",
    "        continue\n",
    "\n",
    "    masks = (pred.masks.data.cpu().numpy() > MASK_THRESHOLD)\n",
    "    cls   = pred.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    # Siapkan overlay keseluruhan sekali saja\n",
    "    overlay = cv2.imread(img_path) if SAVE_OVERLAY else None\n",
    "\n",
    "    leaf_idxs = [i for i, c in enumerate(cls) if c in SEG_LEAF_IDS]\n",
    "    if not leaf_idxs:\n",
    "        rows.append({\"image\": img_name, \"leaf_index\": None, \"leaf_class\": None,\n",
    "                     \"leaf_px\": 0, \"lesion_px\": 0, \"severity_pct\": 0.0})\n",
    "    else:\n",
    "        for k, li in enumerate(leaf_idxs):\n",
    "            leaf_mask   = masks[li]\n",
    "            leaf_class  = cls[li]\n",
    "\n",
    "            lesion_mask = np.zeros_like(leaf_mask, dtype=bool)\n",
    "            if leaf_class in PAIR_LESION_ID:\n",
    "                lesion_id = PAIR_LESION_ID[leaf_class]\n",
    "                lesion_idxs = [i for i, c in enumerate(cls) if c == lesion_id]\n",
    "                if lesion_idxs:\n",
    "                    lesion_mask = np.any(masks[lesion_idxs], axis=0)\n",
    "\n",
    "            lesion_in_leaf = lesion_mask & leaf_mask\n",
    "            leaf_px   = int(leaf_mask.sum())\n",
    "            lesion_px = int(lesion_in_leaf.sum())\n",
    "            sev = (lesion_px / leaf_px * 100.0) if leaf_px > 0 else 0.0\n",
    "\n",
    "            rows.append({\"image\": img_name, \"leaf_index\": k, \"leaf_class\": int(leaf_class),\n",
    "                         \"leaf_px\": leaf_px, \"lesion_px\": lesion_px, \"severity_pct\": sev})\n",
    "\n",
    "            if SAVE_OVERLAY:\n",
    "                overlay[leaf_mask]        = (0.7*overlay[leaf_mask] + 0.3*np.array([0,255,0])).astype(np.uint8)\n",
    "                overlay[lesion_in_leaf]   = (0.7*overlay[lesion_in_leaf] + 0.3*np.array([0,0,255])).astype(np.uint8)\n",
    "\n",
    "    if SAVE_OVERLAY:\n",
    "        cv2.imwrite(str(Path(OUTPUT_DIR) / f\"{Path(img_name).stem}{overlay_suffix}\"), overlay)\n",
    "\n",
    "pd.DataFrame(rows).to_csv(CSV_PATH, index=False)\n",
    "print(f\"[Single-stage Â· all leaves] CSV saved -> {CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9dd05f",
   "metadata": {},
   "source": [
    "### MULTI-STAGE (deteksi âžœ crop âžœ segmen) â€” SATU daun (bbox terbesar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbc8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# MULTI-STAGE Â· BBOX TERBESAR âžœ SEGMENTASI\n",
    "# =========================================\n",
    "from ultralytics import YOLO\n",
    "import numpy as np, cv2, os, glob, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CFG (ISI SENDIRI) ----------\n",
    "DET_MODEL_PATH  = \"PATH/TO/yolo11?-det-best.pt\"  # deteksi: 0=frog-eye-leaf-spot, 1=healthy, 2=rust\n",
    "SEG_MODEL_PATH  = \"PATH/TO/yolo11?-seg-best.pt\"  # segmentasi 5 kelas seperti di atas\n",
    "INPUT_DIR       = \"PATH/TO/images\"\n",
    "OUTPUT_DIR      = \"PATH/TO/output_multi_largest\"\n",
    "CSV_PATH        = str(Path(OUTPUT_DIR) / \"summary_multi_largest.csv\")\n",
    "DET_CONF        = 0.25\n",
    "DET_IOU         = 0.5\n",
    "SEG_CONF        = 0.25\n",
    "SEG_IOU         = 0.5\n",
    "MASK_THRESHOLD  = 0.5\n",
    "PADDING_RATIO   = 0.08   # 8% dari sisi terpanjang bbox\n",
    "SAVE_OVERLAY    = True\n",
    "# ---------------------------------------\n",
    "\n",
    "# Mapping kelas deteksi -> kelas segmen leaf & lesion\n",
    "# det: 0=frog-eye; 1=healthy; 2=rust\n",
    "DET_TO_SEG_LEAF   = {0: 0, 1: 2, 2: 3}\n",
    "DET_TO_SEG_LESION = {0: 1, 2: 4}  # healthy tidak punya lesion\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "model_det = YOLO(DET_MODEL_PATH)\n",
    "model_seg = YOLO(SEG_MODEL_PATH)\n",
    "\n",
    "def clamp(v, lo, hi): return max(lo, min(hi, v))\n",
    "\n",
    "rows = []\n",
    "for img_path in sorted(glob.glob(str(Path(INPUT_DIR) / \"*.*\"))):\n",
    "    img_name = Path(img_path).name\n",
    "    img = cv2.imread(img_path)\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    det = model_det.predict(img_path, conf=DET_CONF, iou=DET_IOU, verbose=False)[0]\n",
    "    if det.boxes is None or len(det.boxes) == 0:\n",
    "        rows.append({\"image\": img_name, \"det_class\": None, \"leaf_px\": 0, \"lesion_px\": 0, \"severity_pct\": 0.0})\n",
    "        continue\n",
    "\n",
    "    boxes = det.boxes.xyxy.cpu().numpy()\n",
    "    dcls  = det.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    areas = (boxes[:,2]-boxes[:,0])*(boxes[:,3]-boxes[:,1])\n",
    "    idx   = int(np.argmax(areas))\n",
    "    x1,y1,x2,y2 = boxes[idx]\n",
    "    det_c = int(dcls[idx])\n",
    "\n",
    "    w, h = x2-x1, y2-y1\n",
    "    pad  = PADDING_RATIO * max(w, h)\n",
    "    x1p, y1p = int(clamp(x1-pad, 0, W-1)), int(clamp(y1-pad, 0, H-1))\n",
    "    x2p, y2p = int(clamp(x2+pad, 0, W-1)), int(clamp(y2+pad, 0, H-1))\n",
    "    crop = img[y1p:y2p, x1p:x2p].copy()\n",
    "\n",
    "    seg = model_seg.predict(crop, conf=SEG_CONF, iou=SEG_IOU, verbose=False)[0]\n",
    "    if seg.masks is None or seg.boxes is None or len(seg.masks) == 0:\n",
    "        rows.append({\"image\": img_name, \"det_class\": det_c, \"leaf_px\": 0, \"lesion_px\": 0, \"severity_pct\": 0.0})\n",
    "        continue\n",
    "\n",
    "    masks = (seg.masks.data.cpu().numpy() > MASK_THRESHOLD)\n",
    "    scls  = seg.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    # pilih leaf sesuai kelas deteksi\n",
    "    seg_leaf_id = DET_TO_SEG_LEAF.get(det_c, None)\n",
    "    leaf_mask = np.zeros_like(masks[0], dtype=bool)\n",
    "    if seg_leaf_id is not None:\n",
    "        leaf_idxs = [i for i, c in enumerate(scls) if c == seg_leaf_id]\n",
    "        if leaf_idxs:\n",
    "            # bila ada banyak instance daun, pilih terbesar\n",
    "            leaf_stack = np.stack([masks[i] for i in leaf_idxs], axis=0)\n",
    "            areas_leaf = leaf_stack.reshape(len(leaf_idxs), -1).sum(axis=1)\n",
    "            leaf_mask  = leaf_stack[int(np.argmax(areas_leaf))]\n",
    "\n",
    "    # lesion sesuai kelas deteksi (healthy tidak punya)\n",
    "    lesion_mask = np.zeros_like(leaf_mask, dtype=bool)\n",
    "    seg_lesion_id = DET_TO_SEG_LESION.get(det_c, None)\n",
    "    if seg_lesion_id is not None:\n",
    "        lesion_idxs = [i for i, c in enumerate(scls) if c == seg_lesion_id]\n",
    "        if lesion_idxs:\n",
    "            lesion_mask = np.any(masks[lesion_idxs], axis=0)\n",
    "\n",
    "    lesion_in_leaf = lesion_mask & leaf_mask\n",
    "    leaf_px   = int(leaf_mask.sum())\n",
    "    lesion_px = int(lesion_in_leaf.sum())\n",
    "    sev = (lesion_px / leaf_px * 100.0) if leaf_px > 0 else 0.0\n",
    "\n",
    "    rows.append({\"image\": img_name, \"det_class\": det_c, \"leaf_px\": leaf_px, \"lesion_px\": lesion_px, \"severity_pct\": sev})\n",
    "\n",
    "    if SAVE_OVERLAY:\n",
    "        overlay = crop.copy()\n",
    "        overlay[leaf_mask]        = (0.7*overlay[leaf_mask] + 0.3*np.array([0,255,0])).astype(np.uint8)\n",
    "        overlay[lesion_in_leaf]   = (0.7*overlay[lesion_in_leaf] + 0.3*np.array([0,0,255])).astype(np.uint8)\n",
    "        cv2.putText(overlay, f\"Severity: {sev:.2f}%\", (10, overlay.shape[0]-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 2, cv2.LINE_AA)\n",
    "        cv2.imwrite(str(Path(OUTPUT_DIR) / f\"{Path(img_name).stem}_largest_overlay.jpg\"), overlay)\n",
    "\n",
    "pd.DataFrame(rows).to_csv(CSV_PATH, index=False)\n",
    "print(f\"[Multi-stage Â· largest] CSV saved -> {CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01858a",
   "metadata": {},
   "source": [
    "### MULTI-STAGE (deteksi âžœ crop âžœ segmen) â€” SEMUA daun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce209ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# MULTI-STAGE Â· SEMUA BBOX âžœ SEGMENTASI\n",
    "# ======================================\n",
    "from ultralytics import YOLO\n",
    "import numpy as np, cv2, os, glob, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CFG (ISI SENDIRI) ----------\n",
    "DET_MODEL_PATH  = \"PATH/TO/yolo11?-det-best.pt\"  # det: 0=frog-eye-leaf-spot,1=healthy,2=rust\n",
    "SEG_MODEL_PATH  = \"PATH/TO/yolo11?-seg-best.pt\"  # seg 5 kelas (lihat atas)\n",
    "INPUT_DIR       = \"PATH/TO/images\"\n",
    "OUTPUT_DIR      = \"PATH/TO/output_multi_all\"\n",
    "CSV_PATH        = str(Path(OUTPUT_DIR) / \"summary_multi_all.csv\")\n",
    "DET_CONF        = 0.25\n",
    "DET_IOU         = 0.5\n",
    "SEG_CONF        = 0.25\n",
    "SEG_IOU         = 0.5\n",
    "MASK_THRESHOLD  = 0.5\n",
    "PADDING_RATIO   = 0.08\n",
    "SAVE_OVERLAY    = True\n",
    "# ---------------------------------------\n",
    "\n",
    "DET_TO_SEG_LEAF   = {0: 0, 1: 2, 2: 3}\n",
    "DET_TO_SEG_LESION = {0: 1, 2: 4}\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "CROP_DIR = Path(OUTPUT_DIR) / \"crops\"\n",
    "os.makedirs(CROP_DIR, exist_ok=True)\n",
    "\n",
    "model_det = YOLO(DET_MODEL_PATH)\n",
    "model_seg = YOLO(SEG_MODEL_PATH)\n",
    "\n",
    "def clamp(v, lo, hi): return max(lo, min(hi, v))\n",
    "\n",
    "rows = []\n",
    "for img_path in sorted(glob.glob(str(Path(INPUT_DIR) / \"*.*\"))):\n",
    "    img_name = Path(img_path).name\n",
    "    img = cv2.imread(img_path)\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    det = model_det.predict(img_path, conf=DET_CONF, iou=DET_IOU, verbose=False)[0]\n",
    "    if det.boxes is None or len(det.boxes) == 0:\n",
    "        rows.append({\"image\": img_name, \"bbox_index\": None, \"det_class\": None,\n",
    "                     \"leaf_px\": 0, \"lesion_px\": 0, \"severity_pct\": 0.0})\n",
    "        continue\n",
    "\n",
    "    boxes = det.boxes.xyxy.cpu().numpy()\n",
    "    dcls  = det.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    for bi, (bb, det_c) in enumerate(zip(boxes, dcls)):\n",
    "        x1,y1,x2,y2 = bb\n",
    "        w, h = x2-x1, y2-y1\n",
    "        pad  = PADDING_RATIO * max(w, h)\n",
    "        x1p, y1p = int(clamp(x1-pad, 0, W-1)), int(clamp(y1-pad, 0, H-1))\n",
    "        x2p, y2p = int(clamp(x2+pad, 0, W-1)), int(clamp(y2+pad, 0, H-1))\n",
    "        crop = img[y1p:y2p, x1p:x2p].copy()\n",
    "\n",
    "        seg = model_seg.predict(crop, conf=SEG_CONF, iou=SEG_IOU, verbose=False)[0]\n",
    "        if seg.masks is None or seg.boxes is None or len(seg.masks) == 0:\n",
    "            rows.append({\"image\": img_name, \"bbox_index\": bi, \"det_class\": int(det_c),\n",
    "                         \"leaf_px\": 0, \"lesion_px\": 0, \"severity_pct\": 0.0})\n",
    "            continue\n",
    "\n",
    "        masks = (seg.masks.data.cpu().numpy() > MASK_THRESHOLD)\n",
    "        scls  = seg.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "        # leaf sesuai kelas deteksi\n",
    "        leaf_mask = np.zeros_like(masks[0], dtype=bool)\n",
    "        seg_leaf_id = DET_TO_SEG_LEAF.get(int(det_c), None)\n",
    "        if seg_leaf_id is not None:\n",
    "            leaf_idxs = [i for i, c in enumerate(scls) if c == seg_leaf_id]\n",
    "            if leaf_idxs:\n",
    "                leaf_stack = np.stack([masks[i] for i in leaf_idxs], axis=0)\n",
    "                areas_leaf = leaf_stack.reshape(len(leaf_idxs), -1).sum(axis=1)\n",
    "                leaf_mask  = leaf_stack[int(np.argmax(areas_leaf))]\n",
    "\n",
    "        # lesion sesuai kelas deteksi (healthy tak punya)\n",
    "        lesion_mask = np.zeros_like(leaf_mask, dtype=bool)\n",
    "        seg_lesion_id = DET_TO_SEG_LESION.get(int(det_c), None)\n",
    "        if seg_lesion_id is not None:\n",
    "            lesion_idxs = [i for i, c in enumerate(scls) if c == seg_lesion_id]\n",
    "            if lesion_idxs:\n",
    "                lesion_mask = np.any(masks[lesion_idxs], axis=0)\n",
    "\n",
    "        lesion_in_leaf = lesion_mask & leaf_mask\n",
    "        leaf_px   = int(leaf_mask.sum())\n",
    "        lesion_px = int(lesion_in_leaf.sum())\n",
    "        sev = (lesion_px / leaf_px * 100.0) if leaf_px > 0 else 0.0\n",
    "\n",
    "        rows.append({\"image\": img_name, \"bbox_index\": bi, \"det_class\": int(det_c),\n",
    "                     \"leaf_px\": leaf_px, \"lesion_px\": lesion_px, \"severity_pct\": sev})\n",
    "\n",
    "        if SAVE_OVERLAY:\n",
    "            overlay = crop.copy()\n",
    "            overlay[leaf_mask]        = (0.7*overlay[leaf_mask] + 0.3*np.array([0,255,0])).astype(np.uint8)\n",
    "            overlay[lesion_in_leaf]   = (0.7*overlay[lesion_in_leaf] + 0.3*np.array([0,0,255])).astype(np.uint8)\n",
    "            cv2.putText(overlay, f\"Severity: {sev:.2f}%\", (10, overlay.shape[0]-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 2, cv2.LINE_AA)\n",
    "            cv2.imwrite(str(Path(CROP_DIR) / f\"{Path(img_name).stem}_bb{bi}_overlay.jpg\"), overlay)\n",
    "\n",
    "pd.DataFrame(rows).to_csv(CSV_PATH, index=False)\n",
    "print(f\"[Multi-stage Â· all leaves] CSV saved -> {CSV_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
